{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "QZCtSJUuCAsz",
    "outputId": "8cfb104f-3b09-4a1f-b924-75748e1ebf3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PadID  Size X  Size Y  Volume  OffsetX  OffsetY  PCB ID  Printing Speed  \\\n",
      "0      1     220     220  74.447   -9.990    5.766     241              30   \n",
      "1      2     220     220  84.224  -15.069   17.116     241              30   \n",
      "2      3     220     220  77.646   -5.895    5.528     241              30   \n",
      "3      4     220     220  79.544  -11.337    8.486     241              30   \n",
      "4      5     220     220  79.548  -13.730    8.963     241              30   \n",
      "\n",
      "   Printing Pressure  Separation Speed Cleaning Type  Cleaning Age Direction  \\\n",
      "0                 60               3.0           Wet             1         F   \n",
      "1                 60               3.0           Wet             1         F   \n",
      "2                 60               3.0           Wet             1         F   \n",
      "3                 60               3.0           Wet             1         F   \n",
      "4                 60               3.0           Wet             1         F   \n",
      "\n",
      "   Pos X    Pos Y  Rotation      AR   ASR  \n",
      "0  31260  40080.0        90  0.6875  2.75  \n",
      "1  31260  39700.0        90  0.6875  2.75  \n",
      "2  31830  40080.0        90  0.6875  2.75  \n",
      "3  31830  39700.0        90  0.6875  2.75  \n",
      "4  32400  40080.0        90  0.6875  2.75  \n",
      "   Volume  OffsetX  OffsetY  PCB ID  Printing Speed  Printing Pressure  \\\n",
      "0  74.447   -9.990    5.766     241              30                 60   \n",
      "1  84.224  -15.069   17.116     241              30                 60   \n",
      "2  77.646   -5.895    5.528     241              30                 60   \n",
      "3  79.544  -11.337    8.486     241              30                 60   \n",
      "4  79.548  -13.730    8.963     241              30                 60   \n",
      "\n",
      "   Separation Speed  Cleaning Type  Cleaning Age  Direction  Pos X    Pos Y  \\\n",
      "0               3.0              1             1          1  31260  40080.0   \n",
      "1               3.0              1             1          1  31260  39700.0   \n",
      "2               3.0              1             1          1  31830  40080.0   \n",
      "3               3.0              1             1          1  31830  39700.0   \n",
      "4               3.0              1             1          1  32400  40080.0   \n",
      "\n",
      "   Rotation  Size  \n",
      "0        90     0  \n",
      "1        90     0  \n",
      "2        90     0  \n",
      "3        90     0  \n",
      "4        90     0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "from scipy.signal import savgol_filter, medfilt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.restoration import denoise_wavelet\n",
    "from cv2 import bilateralFilter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('PrinterSimulatorDataset.csv', low_memory=False)\n",
    "data.columns = data.columns.str.strip()\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# ðŸ‘£ FEATURE ENGINEERING\n",
    "def map_size(x):\n",
    "    if x == 220:\n",
    "        return 'Small'\n",
    "    elif x == 330:\n",
    "        return 'Medium'\n",
    "    elif x == 560:\n",
    "        return 'Large'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "if 'Size X' in data.columns:\n",
    "    data['Size'] = data['Size X'].apply(map_size)\n",
    "    data.drop(columns=['Size X', 'Size Y', 'AR', 'ASR','PadID'], inplace=True, errors='ignore')\n",
    "else:\n",
    "    print(\"Warning: 'Size X' not found.\")\n",
    "    data['Size'] = 'Unknown'\n",
    "\n",
    "size_map = {'Small': 0, 'Medium': 1, 'Large': 2, 'Unknown': -1}\n",
    "data['Size'] = data['Size'].map(size_map)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = ['Cleaning Type', 'Direction', 'Size']\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    if col in data.columns:\n",
    "        data[col] = le.fit_transform(data[col].astype(str))\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# Define features and targets\n",
    "targets = ['Volume', 'OffsetX', 'OffsetY']\n",
    "features = list(data.columns.difference(targets))\n",
    "\n",
    "X = data[features]\n",
    "y = data[targets]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exttZhFjHWYY"
   },
   "source": [
    "***Decision Tree Regressor***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            RMSE                           MAE            \\\n",
      "Target Variable          OffsetX   OffsetY    Volume   OffsetX   OffsetY   \n",
      "Filter + Model                                                             \n",
      "DTCWT + Bilateral       3.531772  3.859028  3.667762  2.729022  2.948271   \n",
      "DTCWT + Gaussian        3.777170  4.099107  3.818528  2.918604  3.139613   \n",
      "DTCWT + Median          4.276956  4.623019  4.225402  3.313960  3.546011   \n",
      "DTCWT + Multivariate    3.469284  3.786472  3.590935  2.678676  2.892800   \n",
      "DTCWT + Savitzky-Golay  3.730837  4.090557  3.897161  2.887450  3.123546   \n",
      "\n",
      "                                  \n",
      "Target Variable           Volume  \n",
      "Filter + Model                    \n",
      "DTCWT + Bilateral       2.776084  \n",
      "DTCWT + Gaussian        2.874029  \n",
      "DTCWT + Median          3.174201  \n",
      "DTCWT + Multivariate    2.712480  \n",
      "DTCWT + Savitzky-Golay  2.966612  \n"
     ]
    }
   ],
   "source": [
    "# Filtering functions\n",
    "def apply_dtcwt_filter(signal):\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=1)\n",
    "    coeffs[1:] = [np.zeros_like(i) for i in coeffs[1:]]\n",
    "    return pywt.waverec(coeffs, 'db4')[:len(signal)]\n",
    "\n",
    "def apply_savgol_filter(signal):\n",
    "    return savgol_filter(signal, window_length=min(len(signal), 21), polyorder=2)\n",
    "\n",
    "def apply_bilateral_filter(signal):\n",
    "    return bilateralFilter(signal.astype(np.float32), 9, 75, 75)\n",
    "\n",
    "def apply_median_filter(signal):\n",
    "    return medfilt(signal, kernel_size=3)\n",
    "\n",
    "def apply_multivariate_filter(signal):\n",
    "    return np.convolve(signal, np.ones(5)/5, mode='same')\n",
    "\n",
    "def apply_gaussian_filter(signal):\n",
    "    return gaussian_filter(signal, sigma=1)\n",
    "# Filtering options\n",
    "filters = {\n",
    "    \"DTCWT + Savitzky-Golay\": lambda x: apply_savgol_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Bilateral\": lambda x: apply_bilateral_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Median\": lambda x: apply_median_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Multivariate\": lambda x: apply_multivariate_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Gaussian\": lambda x: apply_gaussian_filter(apply_dtcwt_filter(x)),\n",
    "}\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "for filter_name, filter_func in filters.items():\n",
    "    filtered_data = y.copy()\n",
    "    for col in targets:\n",
    "        filtered_data[col] = filter_func(y[col])\n",
    "\n",
    "    # Standardization\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    # Train Decision Tree Regressor\n",
    "    for target in targets:\n",
    "        dt_model = DecisionTreeRegressor(random_state=42)\n",
    "        dt_model.fit(X_train_scaled, filtered_data.loc[y_train.index, target])\n",
    "\n",
    "        y_pred_dt = dt_model.predict(X_test_scaled)\n",
    "        rmse_dt = np.sqrt(mean_squared_error(y_test[target], y_pred_dt))\n",
    "        mae_dt = mean_absolute_error(y_test[target], y_pred_dt)\n",
    "\n",
    "        results.append([filter_name, target, rmse_dt, mae_dt])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Filter + Model', 'Target Variable', 'RMSE', 'MAE'])\n",
    "\n",
    "# Print results in a structured way\n",
    "print(results_df.pivot(index='Filter + Model', columns='Target Variable', values=['RMSE', 'MAE']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrewe5MPHRyy"
   },
   "source": [
    "***SVR***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xkw9UgRVBtFe",
    "outputId": "fda2d3da-2302-4eab-b2ac-405f220b2024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            RMSE                           MAE            \\\n",
      "Target Variable          OffsetX   OffsetY    Volume   OffsetX   OffsetY   \n",
      "Filter + Model                                                             \n",
      "DTCWT + Bilateral       0.804924  0.385820  0.840030  0.645018  0.299732   \n",
      "DTCWT + Gaussian        0.809186  0.391201  0.847613  0.648167  0.303763   \n",
      "DTCWT + Median          0.815022  0.396292  0.853232  0.652104  0.307988   \n",
      "DTCWT + Multivariate    0.806419  0.387887  0.843932  0.645995  0.301342   \n",
      "DTCWT + Savitzky-Golay  0.804519  0.385782  0.837915  0.644999  0.299464   \n",
      "\n",
      "                                  \n",
      "Target Variable           Volume  \n",
      "Filter + Model                    \n",
      "DTCWT + Bilateral       0.620134  \n",
      "DTCWT + Gaussian        0.628490  \n",
      "DTCWT + Median          0.635333  \n",
      "DTCWT + Multivariate    0.623838  \n",
      "DTCWT + Savitzky-Golay  0.617852  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Filtering functions\n",
    "def apply_dtcwt_filter(signal):\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=1)\n",
    "    coeffs[1:] = [np.zeros_like(i) for i in coeffs[1:]]\n",
    "    return pywt.waverec(coeffs, 'db4')[:len(signal)]\n",
    "\n",
    "def apply_savgol_filter(signal):\n",
    "    return savgol_filter(signal, window_length=min(len(signal), 21), polyorder=2)\n",
    "\n",
    "def apply_bilateral_filter(signal):\n",
    "    return bilateralFilter(signal.astype(np.float32), 9, 75, 75)\n",
    "\n",
    "def apply_median_filter(signal):\n",
    "    return medfilt(signal, kernel_size=3)\n",
    "\n",
    "def apply_multivariate_filter(signal):\n",
    "    return np.convolve(signal, np.ones(5)/5, mode='same')\n",
    "\n",
    "def apply_gaussian_filter(signal):\n",
    "    return gaussian_filter(signal, sigma=1)\n",
    "\n",
    "# Filtering options\n",
    "filters = {\n",
    "    \"DTCWT + Savitzky-Golay\": lambda x: apply_savgol_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Bilateral\": lambda x: apply_bilateral_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Median\": lambda x: apply_median_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Multivariate\": lambda x: apply_multivariate_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Gaussian\": lambda x: apply_gaussian_filter(apply_dtcwt_filter(x)),\n",
    "}\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "for filter_name, filter_func in filters.items():\n",
    "    filtered_data = y.copy()\n",
    "    for col in targets:\n",
    "        filtered_data[col] = filter_func(y[col])\n",
    "\n",
    "    # Standardization\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_train_scaled = scaler_y.fit_transform(filtered_data.loc[y_train.index])\n",
    "    y_test_scaled = scaler_y.transform(filtered_data.loc[y_test.index])\n",
    "\n",
    "    # Train SVR\n",
    "    for i, target in enumerate(targets):\n",
    "        svr_model = LinearSVR(max_iter=1000, random_state=42)\n",
    "        svr_model.fit(X_train_scaled, y_train_scaled[:, i])\n",
    "\n",
    "        y_pred_svr = svr_model.predict(X_test_scaled)\n",
    "        rmse_svr = np.sqrt(mean_squared_error(y_test_scaled[:, i], y_pred_svr))\n",
    "        mae_svr = mean_absolute_error(y_test_scaled[:, i], y_pred_svr)\n",
    "\n",
    "        results.append([filter_name, target, rmse_svr, mae_svr])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Filter + Model', 'Target Variable', 'RMSE', 'MAE'])\n",
    "\n",
    "# Print results in a structured way\n",
    "print(results_df.pivot(index='Filter + Model', columns='Target Variable', values=['RMSE', 'MAE']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPFuMnBgHb8z"
   },
   "source": [
    "***ANN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_7vHrwVChPBp",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.19355937\n",
      "Iteration 2, loss = 0.12146921\n",
      "Iteration 3, loss = 0.10897817\n",
      "Iteration 4, loss = 0.10382465\n",
      "Iteration 5, loss = 0.10041296\n",
      "Iteration 6, loss = 0.09820451\n",
      "Iteration 7, loss = 0.09666824\n",
      "Iteration 8, loss = 0.09523918\n",
      "Iteration 9, loss = 0.09421598\n",
      "Iteration 10, loss = 0.09310146\n",
      "Iteration 11, loss = 0.09235230\n",
      "Iteration 12, loss = 0.09158040\n",
      "Iteration 13, loss = 0.09087600\n",
      "Iteration 14, loss = 0.09019016\n",
      "Iteration 15, loss = 0.08962413\n",
      "Iteration 16, loss = 0.08918563\n",
      "Iteration 17, loss = 0.08847256\n",
      "Iteration 18, loss = 0.08795073\n",
      "Iteration 19, loss = 0.08759927\n",
      "Iteration 20, loss = 0.08725159\n",
      "Iteration 21, loss = 0.08692008\n",
      "Iteration 22, loss = 0.08649023\n",
      "Iteration 23, loss = 0.08639132\n",
      "Iteration 24, loss = 0.08597669\n",
      "Iteration 25, loss = 0.08592693\n",
      "Iteration 26, loss = 0.08550930\n",
      "Iteration 27, loss = 0.08522695\n",
      "Iteration 28, loss = 0.08508793\n",
      "Iteration 29, loss = 0.08494898\n",
      "Iteration 30, loss = 0.08466175\n",
      "Iteration 31, loss = 0.08449946\n",
      "Iteration 32, loss = 0.08435966\n",
      "Iteration 33, loss = 0.08418948\n",
      "Iteration 34, loss = 0.08400700\n",
      "Iteration 35, loss = 0.08394033\n",
      "Iteration 36, loss = 0.08366436\n",
      "Iteration 37, loss = 0.08354748\n",
      "Iteration 38, loss = 0.08333384\n",
      "Iteration 39, loss = 0.08330331\n",
      "Iteration 40, loss = 0.08315178\n",
      "Iteration 41, loss = 0.08305373\n",
      "Iteration 42, loss = 0.08284180\n",
      "Iteration 43, loss = 0.08265667\n",
      "Iteration 44, loss = 0.08259013\n",
      "Iteration 45, loss = 0.08250892\n",
      "Iteration 46, loss = 0.08246135\n",
      "Iteration 47, loss = 0.08227005\n",
      "Iteration 48, loss = 0.08226492\n",
      "Iteration 49, loss = 0.08207895\n",
      "Iteration 50, loss = 0.08207724\n",
      "Iteration 51, loss = 0.08186433\n",
      "Iteration 52, loss = 0.08191226\n",
      "Iteration 53, loss = 0.08169646\n",
      "Iteration 54, loss = 0.08165154\n",
      "Iteration 55, loss = 0.08161313\n",
      "Iteration 56, loss = 0.08156722\n",
      "Iteration 57, loss = 0.08145641\n",
      "Iteration 58, loss = 0.08133581\n",
      "Iteration 59, loss = 0.08113606\n",
      "Iteration 60, loss = 0.08124975\n",
      "Iteration 61, loss = 0.08112470\n",
      "Iteration 62, loss = 0.08094075\n",
      "Iteration 63, loss = 0.08074387\n",
      "Iteration 64, loss = 0.08082215\n",
      "Iteration 65, loss = 0.08084504\n",
      "Iteration 66, loss = 0.08067648\n",
      "Iteration 67, loss = 0.08061227\n",
      "Iteration 68, loss = 0.08060824\n",
      "Iteration 69, loss = 0.08046250\n",
      "Iteration 70, loss = 0.08045736\n",
      "Iteration 71, loss = 0.08029390\n",
      "Iteration 72, loss = 0.08028579\n",
      "Iteration 73, loss = 0.08024130\n",
      "Iteration 74, loss = 0.08009431\n",
      "Iteration 75, loss = 0.07997295\n",
      "Iteration 76, loss = 0.07994819\n",
      "Iteration 77, loss = 0.07993296\n",
      "Iteration 78, loss = 0.07988002\n",
      "Iteration 79, loss = 0.07974539\n",
      "Iteration 80, loss = 0.07965032\n",
      "Iteration 81, loss = 0.07977650\n",
      "Iteration 82, loss = 0.07952044\n",
      "Iteration 83, loss = 0.07961945\n",
      "Iteration 84, loss = 0.07956549\n",
      "Iteration 85, loss = 0.07946744\n",
      "Iteration 86, loss = 0.07946136\n",
      "Iteration 87, loss = 0.07926733\n",
      "Iteration 88, loss = 0.07942608\n",
      "Iteration 89, loss = 0.07912649\n",
      "Iteration 90, loss = 0.07932159\n",
      "Iteration 91, loss = 0.07920218\n",
      "Iteration 92, loss = 0.07921167\n",
      "Iteration 93, loss = 0.07914093\n",
      "Iteration 94, loss = 0.07913583\n",
      "Iteration 95, loss = 0.07900816\n",
      "Iteration 96, loss = 0.07898983\n",
      "Iteration 97, loss = 0.07898720\n",
      "Iteration 98, loss = 0.07890430\n",
      "Iteration 99, loss = 0.07884094\n",
      "Iteration 100, loss = 0.07878195\n",
      "Iteration 1, loss = 0.14339673\n",
      "Iteration 2, loss = 0.06143194\n",
      "Iteration 3, loss = 0.05065629\n",
      "Iteration 4, loss = 0.04644371\n",
      "Iteration 5, loss = 0.04404087\n",
      "Iteration 6, loss = 0.04228884\n",
      "Iteration 7, loss = 0.04084699\n",
      "Iteration 8, loss = 0.03999971\n",
      "Iteration 9, loss = 0.03929635\n",
      "Iteration 10, loss = 0.03872057\n",
      "Iteration 11, loss = 0.03827931\n",
      "Iteration 12, loss = 0.03784953\n",
      "Iteration 13, loss = 0.03750603\n",
      "Iteration 14, loss = 0.03711918\n",
      "Iteration 15, loss = 0.03678201\n",
      "Iteration 16, loss = 0.03649163\n",
      "Iteration 17, loss = 0.03632710\n",
      "Iteration 18, loss = 0.03620538\n",
      "Iteration 19, loss = 0.03591913\n",
      "Iteration 20, loss = 0.03576586\n",
      "Iteration 21, loss = 0.03557390\n",
      "Iteration 22, loss = 0.03542182\n",
      "Iteration 23, loss = 0.03518820\n",
      "Iteration 24, loss = 0.03489934\n",
      "Iteration 25, loss = 0.03466853\n",
      "Iteration 26, loss = 0.03440147\n",
      "Iteration 27, loss = 0.03422009\n",
      "Iteration 28, loss = 0.03391838\n",
      "Iteration 29, loss = 0.03362559\n",
      "Iteration 30, loss = 0.03349789\n",
      "Iteration 31, loss = 0.03338448\n",
      "Iteration 32, loss = 0.03317465\n",
      "Iteration 33, loss = 0.03318191\n",
      "Iteration 34, loss = 0.03299040\n",
      "Iteration 35, loss = 0.03293620\n",
      "Iteration 36, loss = 0.03282631\n",
      "Iteration 37, loss = 0.03277574\n",
      "Iteration 38, loss = 0.03264452\n",
      "Iteration 39, loss = 0.03235526\n",
      "Iteration 40, loss = 0.03217244\n",
      "Iteration 41, loss = 0.03208753\n",
      "Iteration 42, loss = 0.03200897\n",
      "Iteration 43, loss = 0.03194761\n",
      "Iteration 44, loss = 0.03188613\n",
      "Iteration 45, loss = 0.03180549\n",
      "Iteration 46, loss = 0.03176316\n",
      "Iteration 47, loss = 0.03170609\n",
      "Iteration 48, loss = 0.03168555\n",
      "Iteration 49, loss = 0.03162781\n",
      "Iteration 50, loss = 0.03152175\n",
      "Iteration 51, loss = 0.03155567\n",
      "Iteration 52, loss = 0.03154579\n",
      "Iteration 53, loss = 0.03146644\n",
      "Iteration 54, loss = 0.03141435\n",
      "Iteration 55, loss = 0.03140322\n",
      "Iteration 56, loss = 0.03135110\n",
      "Iteration 57, loss = 0.03127606\n",
      "Iteration 58, loss = 0.03126313\n",
      "Iteration 59, loss = 0.03128303\n",
      "Iteration 60, loss = 0.03120497\n",
      "Iteration 61, loss = 0.03109748\n",
      "Iteration 62, loss = 0.03110738\n",
      "Iteration 63, loss = 0.03106780\n",
      "Iteration 64, loss = 0.03102735\n",
      "Iteration 65, loss = 0.03100157\n",
      "Iteration 66, loss = 0.03097876\n",
      "Iteration 67, loss = 0.03096006\n",
      "Iteration 68, loss = 0.03093893\n",
      "Iteration 69, loss = 0.03091309\n",
      "Iteration 70, loss = 0.03084855\n",
      "Iteration 71, loss = 0.03084689\n",
      "Iteration 72, loss = 0.03081749\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03453839\n",
      "Iteration 2, loss = 0.01334253\n",
      "Iteration 3, loss = 0.01102580\n",
      "Iteration 4, loss = 0.01015054\n",
      "Iteration 5, loss = 0.00963468\n",
      "Iteration 6, loss = 0.00932516\n",
      "Iteration 7, loss = 0.00911035\n",
      "Iteration 8, loss = 0.00892282\n",
      "Iteration 9, loss = 0.00877860\n",
      "Iteration 10, loss = 0.00862262\n",
      "Iteration 11, loss = 0.00851386\n",
      "Iteration 12, loss = 0.00839310\n",
      "Iteration 13, loss = 0.00830034\n",
      "Iteration 14, loss = 0.00816434\n",
      "Iteration 15, loss = 0.00812957\n",
      "Iteration 16, loss = 0.00803928\n",
      "Iteration 17, loss = 0.00800438\n",
      "Iteration 18, loss = 0.00795265\n",
      "Iteration 19, loss = 0.00790115\n",
      "Iteration 20, loss = 0.00784938\n",
      "Iteration 21, loss = 0.00782330\n",
      "Iteration 22, loss = 0.00777925\n",
      "Iteration 23, loss = 0.00776964\n",
      "Iteration 24, loss = 0.00773318\n",
      "Iteration 25, loss = 0.00770691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19756453\n",
      "Iteration 2, loss = 0.12812610\n",
      "Iteration 3, loss = 0.11639278\n",
      "Iteration 4, loss = 0.11104781\n",
      "Iteration 5, loss = 0.10764308\n",
      "Iteration 6, loss = 0.10553895\n",
      "Iteration 7, loss = 0.10377462\n",
      "Iteration 8, loss = 0.10224411\n",
      "Iteration 9, loss = 0.10111583\n",
      "Iteration 10, loss = 0.09990059\n",
      "Iteration 11, loss = 0.09884406\n",
      "Iteration 12, loss = 0.09816327\n",
      "Iteration 13, loss = 0.09746811\n",
      "Iteration 14, loss = 0.09686556\n",
      "Iteration 15, loss = 0.09634122\n",
      "Iteration 16, loss = 0.09590159\n",
      "Iteration 17, loss = 0.09542293\n",
      "Iteration 18, loss = 0.09498261\n",
      "Iteration 19, loss = 0.09460194\n",
      "Iteration 20, loss = 0.09429703\n",
      "Iteration 21, loss = 0.09403958\n",
      "Iteration 22, loss = 0.09350606\n",
      "Iteration 23, loss = 0.09349228\n",
      "Iteration 24, loss = 0.09307609\n",
      "Iteration 25, loss = 0.09301596\n",
      "Iteration 26, loss = 0.09258947\n",
      "Iteration 27, loss = 0.09228185\n",
      "Iteration 28, loss = 0.09210289\n",
      "Iteration 29, loss = 0.09188651\n",
      "Iteration 30, loss = 0.09158570\n",
      "Iteration 31, loss = 0.09146736\n",
      "Iteration 32, loss = 0.09129350\n",
      "Iteration 33, loss = 0.09112881\n",
      "Iteration 34, loss = 0.09097394\n",
      "Iteration 35, loss = 0.09089024\n",
      "Iteration 36, loss = 0.09060243\n",
      "Iteration 37, loss = 0.09043170\n",
      "Iteration 38, loss = 0.09024352\n",
      "Iteration 39, loss = 0.09032592\n",
      "Iteration 40, loss = 0.09009605\n",
      "Iteration 41, loss = 0.08997676\n",
      "Iteration 42, loss = 0.08967661\n",
      "Iteration 43, loss = 0.08959089\n",
      "Iteration 44, loss = 0.08950033\n",
      "Iteration 45, loss = 0.08944303\n",
      "Iteration 46, loss = 0.08924897\n",
      "Iteration 47, loss = 0.08920709\n",
      "Iteration 48, loss = 0.08920622\n",
      "Iteration 49, loss = 0.08893397\n",
      "Iteration 50, loss = 0.08888345\n",
      "Iteration 51, loss = 0.08876543\n",
      "Iteration 52, loss = 0.08869780\n",
      "Iteration 53, loss = 0.08866386\n",
      "Iteration 54, loss = 0.08848387\n",
      "Iteration 55, loss = 0.08844330\n",
      "Iteration 56, loss = 0.08844274\n",
      "Iteration 57, loss = 0.08826711\n",
      "Iteration 58, loss = 0.08822128\n",
      "Iteration 59, loss = 0.08801154\n",
      "Iteration 60, loss = 0.08809767\n",
      "Iteration 61, loss = 0.08800111\n",
      "Iteration 62, loss = 0.08776838\n",
      "Iteration 63, loss = 0.08768167\n",
      "Iteration 64, loss = 0.08777714\n",
      "Iteration 65, loss = 0.08777335\n",
      "Iteration 66, loss = 0.08755487\n",
      "Iteration 67, loss = 0.08755481\n",
      "Iteration 68, loss = 0.08755546\n",
      "Iteration 69, loss = 0.08744357\n",
      "Iteration 70, loss = 0.08733805\n",
      "Iteration 71, loss = 0.08727075\n",
      "Iteration 72, loss = 0.08722255\n",
      "Iteration 73, loss = 0.08717049\n",
      "Iteration 74, loss = 0.08702450\n",
      "Iteration 75, loss = 0.08691921\n",
      "Iteration 76, loss = 0.08690112\n",
      "Iteration 77, loss = 0.08692167\n",
      "Iteration 78, loss = 0.08684635\n",
      "Iteration 79, loss = 0.08674800\n",
      "Iteration 80, loss = 0.08672905\n",
      "Iteration 81, loss = 0.08685352\n",
      "Iteration 82, loss = 0.08662545\n",
      "Iteration 83, loss = 0.08662136\n",
      "Iteration 84, loss = 0.08652052\n",
      "Iteration 85, loss = 0.08647906\n",
      "Iteration 86, loss = 0.08654210\n",
      "Iteration 87, loss = 0.08623712\n",
      "Iteration 88, loss = 0.08648992\n",
      "Iteration 89, loss = 0.08624820\n",
      "Iteration 90, loss = 0.08629326\n",
      "Iteration 91, loss = 0.08622685\n",
      "Iteration 92, loss = 0.08625447\n",
      "Iteration 93, loss = 0.08617737\n",
      "Iteration 94, loss = 0.08615397\n",
      "Iteration 95, loss = 0.08607722\n",
      "Iteration 96, loss = 0.08602623\n",
      "Iteration 97, loss = 0.08608088\n",
      "Iteration 98, loss = 0.08597259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.14386957\n",
      "Iteration 2, loss = 0.05935616\n",
      "Iteration 3, loss = 0.04951589\n",
      "Iteration 4, loss = 0.04527301\n",
      "Iteration 5, loss = 0.04253510\n",
      "Iteration 6, loss = 0.04093051\n",
      "Iteration 7, loss = 0.03988697\n",
      "Iteration 8, loss = 0.03920419\n",
      "Iteration 9, loss = 0.03860480\n",
      "Iteration 10, loss = 0.03808535\n",
      "Iteration 11, loss = 0.03759974\n",
      "Iteration 12, loss = 0.03691894\n",
      "Iteration 13, loss = 0.03625198\n",
      "Iteration 14, loss = 0.03566452\n",
      "Iteration 15, loss = 0.03528721\n",
      "Iteration 16, loss = 0.03446953\n",
      "Iteration 17, loss = 0.03425200\n",
      "Iteration 18, loss = 0.03395178\n",
      "Iteration 19, loss = 0.03381961\n",
      "Iteration 20, loss = 0.03359241\n",
      "Iteration 21, loss = 0.03342292\n",
      "Iteration 22, loss = 0.03335672\n",
      "Iteration 23, loss = 0.03316746\n",
      "Iteration 24, loss = 0.03305435\n",
      "Iteration 25, loss = 0.03299109\n",
      "Iteration 26, loss = 0.03288085\n",
      "Iteration 27, loss = 0.03282355\n",
      "Iteration 28, loss = 0.03270501\n",
      "Iteration 29, loss = 0.03267985\n",
      "Iteration 30, loss = 0.03258794\n",
      "Iteration 31, loss = 0.03254239\n",
      "Iteration 32, loss = 0.03242191\n",
      "Iteration 33, loss = 0.03244164\n",
      "Iteration 34, loss = 0.03235419\n",
      "Iteration 35, loss = 0.03221902\n",
      "Iteration 36, loss = 0.03221473\n",
      "Iteration 37, loss = 0.03215617\n",
      "Iteration 38, loss = 0.03209656\n",
      "Iteration 39, loss = 0.03204459\n",
      "Iteration 40, loss = 0.03198816\n",
      "Iteration 41, loss = 0.03199402\n",
      "Iteration 42, loss = 0.03186899\n",
      "Iteration 43, loss = 0.03184435\n",
      "Iteration 44, loss = 0.03180265\n",
      "Iteration 45, loss = 0.03176523\n",
      "Iteration 46, loss = 0.03173524\n",
      "Iteration 47, loss = 0.03164565\n",
      "Iteration 48, loss = 0.03166720\n",
      "Iteration 49, loss = 0.03153819\n",
      "Iteration 50, loss = 0.03147724\n",
      "Iteration 51, loss = 0.03151048\n",
      "Iteration 52, loss = 0.03149133\n",
      "Iteration 53, loss = 0.03146420\n",
      "Iteration 54, loss = 0.03138549\n",
      "Iteration 55, loss = 0.03137191\n",
      "Iteration 56, loss = 0.03137428\n",
      "Iteration 57, loss = 0.03129654\n",
      "Iteration 58, loss = 0.03124279\n",
      "Iteration 59, loss = 0.03134357\n",
      "Iteration 60, loss = 0.03118009\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03440194\n",
      "Iteration 2, loss = 0.01313563\n",
      "Iteration 3, loss = 0.01083642\n",
      "Iteration 4, loss = 0.01005760\n",
      "Iteration 5, loss = 0.00958919\n",
      "Iteration 6, loss = 0.00928928\n",
      "Iteration 7, loss = 0.00908882\n",
      "Iteration 8, loss = 0.00888345\n",
      "Iteration 9, loss = 0.00869533\n",
      "Iteration 10, loss = 0.00857688\n",
      "Iteration 11, loss = 0.00848564\n",
      "Iteration 12, loss = 0.00839127\n",
      "Iteration 13, loss = 0.00832666\n",
      "Iteration 14, loss = 0.00824679\n",
      "Iteration 15, loss = 0.00821029\n",
      "Iteration 16, loss = 0.00814308\n",
      "Iteration 17, loss = 0.00810939\n",
      "Iteration 18, loss = 0.00806715\n",
      "Iteration 19, loss = 0.00801614\n",
      "Iteration 20, loss = 0.00799530\n",
      "Iteration 21, loss = 0.00797064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22799135\n",
      "Iteration 2, loss = 0.16578963\n",
      "Iteration 3, loss = 0.15258081\n",
      "Iteration 4, loss = 0.14702708\n",
      "Iteration 5, loss = 0.14367080\n",
      "Iteration 6, loss = 0.14134449\n",
      "Iteration 7, loss = 0.13944571\n",
      "Iteration 8, loss = 0.13780629\n",
      "Iteration 9, loss = 0.13639608\n",
      "Iteration 10, loss = 0.13533200\n",
      "Iteration 11, loss = 0.13440819\n",
      "Iteration 12, loss = 0.13345582\n",
      "Iteration 13, loss = 0.13268439\n",
      "Iteration 14, loss = 0.13200065\n",
      "Iteration 15, loss = 0.13152398\n",
      "Iteration 16, loss = 0.13106225\n",
      "Iteration 17, loss = 0.13049644\n",
      "Iteration 18, loss = 0.12998390\n",
      "Iteration 19, loss = 0.12964956\n",
      "Iteration 20, loss = 0.12925415\n",
      "Iteration 21, loss = 0.12891769\n",
      "Iteration 22, loss = 0.12847066\n",
      "Iteration 23, loss = 0.12840999\n",
      "Iteration 24, loss = 0.12798100\n",
      "Iteration 25, loss = 0.12783728\n",
      "Iteration 26, loss = 0.12733288\n",
      "Iteration 27, loss = 0.12708682\n",
      "Iteration 28, loss = 0.12679497\n",
      "Iteration 29, loss = 0.12651579\n",
      "Iteration 30, loss = 0.12632901\n",
      "Iteration 31, loss = 0.12625107\n",
      "Iteration 32, loss = 0.12603063\n",
      "Iteration 33, loss = 0.12571084\n",
      "Iteration 34, loss = 0.12561927\n",
      "Iteration 35, loss = 0.12550002\n",
      "Iteration 36, loss = 0.12522252\n",
      "Iteration 37, loss = 0.12509674\n",
      "Iteration 38, loss = 0.12487248\n",
      "Iteration 39, loss = 0.12490874\n",
      "Iteration 40, loss = 0.12468417\n",
      "Iteration 41, loss = 0.12455307\n",
      "Iteration 42, loss = 0.12432848\n",
      "Iteration 43, loss = 0.12424001\n",
      "Iteration 44, loss = 0.12418135\n",
      "Iteration 45, loss = 0.12413831\n",
      "Iteration 46, loss = 0.12386205\n",
      "Iteration 47, loss = 0.12369965\n",
      "Iteration 48, loss = 0.12368879\n",
      "Iteration 49, loss = 0.12340021\n",
      "Iteration 50, loss = 0.12326448\n",
      "Iteration 51, loss = 0.12314087\n",
      "Iteration 52, loss = 0.12303602\n",
      "Iteration 53, loss = 0.12302591\n",
      "Iteration 54, loss = 0.12275185\n",
      "Iteration 55, loss = 0.12275140\n",
      "Iteration 56, loss = 0.12265716\n",
      "Iteration 57, loss = 0.12253516\n",
      "Iteration 58, loss = 0.12254036\n",
      "Iteration 59, loss = 0.12220006\n",
      "Iteration 60, loss = 0.12219896\n",
      "Iteration 61, loss = 0.12228081\n",
      "Iteration 62, loss = 0.12191119\n",
      "Iteration 63, loss = 0.12188995\n",
      "Iteration 64, loss = 0.12188372\n",
      "Iteration 65, loss = 0.12191102\n",
      "Iteration 66, loss = 0.12162787\n",
      "Iteration 67, loss = 0.12170402\n",
      "Iteration 68, loss = 0.12168547\n",
      "Iteration 69, loss = 0.12153493\n",
      "Iteration 70, loss = 0.12136152\n",
      "Iteration 71, loss = 0.12131389\n",
      "Iteration 72, loss = 0.12127846\n",
      "Iteration 73, loss = 0.12136007\n",
      "Iteration 74, loss = 0.12123981\n",
      "Iteration 75, loss = 0.12099336\n",
      "Iteration 76, loss = 0.12099723\n",
      "Iteration 77, loss = 0.12107101\n",
      "Iteration 78, loss = 0.12097682\n",
      "Iteration 79, loss = 0.12094385\n",
      "Iteration 80, loss = 0.12094892\n",
      "Iteration 81, loss = 0.12094104\n",
      "Iteration 82, loss = 0.12082771\n",
      "Iteration 83, loss = 0.12072162\n",
      "Iteration 84, loss = 0.12061146\n",
      "Iteration 85, loss = 0.12062364\n",
      "Iteration 86, loss = 0.12066970\n",
      "Iteration 87, loss = 0.12046412\n",
      "Iteration 88, loss = 0.12055713\n",
      "Iteration 89, loss = 0.12049968\n",
      "Iteration 90, loss = 0.12045769\n",
      "Iteration 91, loss = 0.12035437\n",
      "Iteration 92, loss = 0.12045137\n",
      "Iteration 93, loss = 0.12026291\n",
      "Iteration 94, loss = 0.12022430\n",
      "Iteration 95, loss = 0.12024750\n",
      "Iteration 96, loss = 0.12025112\n",
      "Iteration 97, loss = 0.12020894\n",
      "Iteration 98, loss = 0.12014352\n",
      "Iteration 99, loss = 0.12011562\n",
      "Iteration 100, loss = 0.11998016\n",
      "Iteration 1, loss = 0.16389798\n",
      "Iteration 2, loss = 0.08559974\n",
      "Iteration 3, loss = 0.07595745\n",
      "Iteration 4, loss = 0.07230906\n",
      "Iteration 5, loss = 0.06977356\n",
      "Iteration 6, loss = 0.06806595\n",
      "Iteration 7, loss = 0.06654428\n",
      "Iteration 8, loss = 0.06558958\n",
      "Iteration 9, loss = 0.06478556\n",
      "Iteration 10, loss = 0.06417767\n",
      "Iteration 11, loss = 0.06365028\n",
      "Iteration 12, loss = 0.06307297\n",
      "Iteration 13, loss = 0.06268768\n",
      "Iteration 14, loss = 0.06215071\n",
      "Iteration 15, loss = 0.06173689\n",
      "Iteration 16, loss = 0.06132696\n",
      "Iteration 17, loss = 0.06091436\n",
      "Iteration 18, loss = 0.06048191\n",
      "Iteration 19, loss = 0.06021931\n",
      "Iteration 20, loss = 0.05987663\n",
      "Iteration 21, loss = 0.05947373\n",
      "Iteration 22, loss = 0.05879400\n",
      "Iteration 23, loss = 0.05820164\n",
      "Iteration 24, loss = 0.05780958\n",
      "Iteration 25, loss = 0.05745627\n",
      "Iteration 26, loss = 0.05722425\n",
      "Iteration 27, loss = 0.05696111\n",
      "Iteration 28, loss = 0.05679741\n",
      "Iteration 29, loss = 0.05661694\n",
      "Iteration 30, loss = 0.05645982\n",
      "Iteration 31, loss = 0.05640931\n",
      "Iteration 32, loss = 0.05622197\n",
      "Iteration 33, loss = 0.05612028\n",
      "Iteration 34, loss = 0.05604991\n",
      "Iteration 35, loss = 0.05593830\n",
      "Iteration 36, loss = 0.05592797\n",
      "Iteration 37, loss = 0.05574525\n",
      "Iteration 38, loss = 0.05566031\n",
      "Iteration 39, loss = 0.05565425\n",
      "Iteration 40, loss = 0.05557977\n",
      "Iteration 41, loss = 0.05550632\n",
      "Iteration 42, loss = 0.05541017\n",
      "Iteration 43, loss = 0.05541695\n",
      "Iteration 44, loss = 0.05524842\n",
      "Iteration 45, loss = 0.05517689\n",
      "Iteration 46, loss = 0.05514671\n",
      "Iteration 47, loss = 0.05506119\n",
      "Iteration 48, loss = 0.05510054\n",
      "Iteration 49, loss = 0.05498515\n",
      "Iteration 50, loss = 0.05491668\n",
      "Iteration 51, loss = 0.05479936\n",
      "Iteration 52, loss = 0.05474742\n",
      "Iteration 53, loss = 0.05472613\n",
      "Iteration 54, loss = 0.05469660\n",
      "Iteration 55, loss = 0.05464901\n",
      "Iteration 56, loss = 0.05457266\n",
      "Iteration 57, loss = 0.05450708\n",
      "Iteration 58, loss = 0.05452324\n",
      "Iteration 59, loss = 0.05454964\n",
      "Iteration 60, loss = 0.05437381\n",
      "Iteration 61, loss = 0.05423510\n",
      "Iteration 62, loss = 0.05434306\n",
      "Iteration 63, loss = 0.05419050\n",
      "Iteration 64, loss = 0.05424381\n",
      "Iteration 65, loss = 0.05425758\n",
      "Iteration 66, loss = 0.05416402\n",
      "Iteration 67, loss = 0.05414998\n",
      "Iteration 68, loss = 0.05410961\n",
      "Iteration 69, loss = 0.05401702\n",
      "Iteration 70, loss = 0.05401038\n",
      "Iteration 71, loss = 0.05404844\n",
      "Iteration 72, loss = 0.05389074\n",
      "Iteration 73, loss = 0.05392905\n",
      "Iteration 74, loss = 0.05386299\n",
      "Iteration 75, loss = 0.05393702\n",
      "Iteration 76, loss = 0.05385237\n",
      "Iteration 77, loss = 0.05373691\n",
      "Iteration 78, loss = 0.05380725\n",
      "Iteration 79, loss = 0.05364218\n",
      "Iteration 80, loss = 0.05367781\n",
      "Iteration 81, loss = 0.05368976\n",
      "Iteration 82, loss = 0.05371571\n",
      "Iteration 83, loss = 0.05357846\n",
      "Iteration 84, loss = 0.05362887\n",
      "Iteration 85, loss = 0.05363802\n",
      "Iteration 86, loss = 0.05353581\n",
      "Iteration 87, loss = 0.05356779\n",
      "Iteration 88, loss = 0.05354778\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03931040\n",
      "Iteration 2, loss = 0.01838350\n",
      "Iteration 3, loss = 0.01605485\n",
      "Iteration 4, loss = 0.01513196\n",
      "Iteration 5, loss = 0.01457844\n",
      "Iteration 6, loss = 0.01426056\n",
      "Iteration 7, loss = 0.01403779\n",
      "Iteration 8, loss = 0.01386554\n",
      "Iteration 9, loss = 0.01367222\n",
      "Iteration 10, loss = 0.01357347\n",
      "Iteration 11, loss = 0.01346571\n",
      "Iteration 12, loss = 0.01339345\n",
      "Iteration 13, loss = 0.01328733\n",
      "Iteration 14, loss = 0.01323067\n",
      "Iteration 15, loss = 0.01316197\n",
      "Iteration 16, loss = 0.01311482\n",
      "Iteration 17, loss = 0.01306329\n",
      "Iteration 18, loss = 0.01300955\n",
      "Iteration 19, loss = 0.01298028\n",
      "Iteration 20, loss = 0.01292906\n",
      "Iteration 21, loss = 0.01287807\n",
      "Iteration 22, loss = 0.01285752\n",
      "Iteration 23, loss = 0.01282890\n",
      "Iteration 24, loss = 0.01278798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20530539\n",
      "Iteration 2, loss = 0.13986989\n",
      "Iteration 3, loss = 0.12740955\n",
      "Iteration 4, loss = 0.12143730\n",
      "Iteration 5, loss = 0.11799815\n",
      "Iteration 6, loss = 0.11544391\n",
      "Iteration 7, loss = 0.11373795\n",
      "Iteration 8, loss = 0.11213957\n",
      "Iteration 9, loss = 0.11080948\n",
      "Iteration 10, loss = 0.10964343\n",
      "Iteration 11, loss = 0.10858776\n",
      "Iteration 12, loss = 0.10783778\n",
      "Iteration 13, loss = 0.10718382\n",
      "Iteration 14, loss = 0.10657861\n",
      "Iteration 15, loss = 0.10612337\n",
      "Iteration 16, loss = 0.10562991\n",
      "Iteration 17, loss = 0.10510565\n",
      "Iteration 18, loss = 0.10460613\n",
      "Iteration 19, loss = 0.10429680\n",
      "Iteration 20, loss = 0.10387515\n",
      "Iteration 21, loss = 0.10362486\n",
      "Iteration 22, loss = 0.10307121\n",
      "Iteration 23, loss = 0.10300918\n",
      "Iteration 24, loss = 0.10258853\n",
      "Iteration 25, loss = 0.10246114\n",
      "Iteration 26, loss = 0.10196752\n",
      "Iteration 27, loss = 0.10154739\n",
      "Iteration 28, loss = 0.10133001\n",
      "Iteration 29, loss = 0.10109253\n",
      "Iteration 30, loss = 0.10077045\n",
      "Iteration 31, loss = 0.10050906\n",
      "Iteration 32, loss = 0.10034562\n",
      "Iteration 33, loss = 0.10016525\n",
      "Iteration 34, loss = 0.09993474\n",
      "Iteration 35, loss = 0.09989673\n",
      "Iteration 36, loss = 0.09959025\n",
      "Iteration 37, loss = 0.09930740\n",
      "Iteration 38, loss = 0.09927270\n",
      "Iteration 39, loss = 0.09917141\n",
      "Iteration 40, loss = 0.09886180\n",
      "Iteration 41, loss = 0.09873686\n",
      "Iteration 42, loss = 0.09867900\n",
      "Iteration 43, loss = 0.09848403\n",
      "Iteration 44, loss = 0.09841705\n",
      "Iteration 45, loss = 0.09824322\n",
      "Iteration 46, loss = 0.09820036\n",
      "Iteration 47, loss = 0.09804354\n",
      "Iteration 48, loss = 0.09800702\n",
      "Iteration 49, loss = 0.09786449\n",
      "Iteration 50, loss = 0.09765610\n",
      "Iteration 51, loss = 0.09757428\n",
      "Iteration 52, loss = 0.09750462\n",
      "Iteration 53, loss = 0.09753958\n",
      "Iteration 54, loss = 0.09728244\n",
      "Iteration 55, loss = 0.09726818\n",
      "Iteration 56, loss = 0.09723841\n",
      "Iteration 57, loss = 0.09711564\n",
      "Iteration 58, loss = 0.09705314\n",
      "Iteration 59, loss = 0.09676777\n",
      "Iteration 60, loss = 0.09691718\n",
      "Iteration 61, loss = 0.09678513\n",
      "Iteration 62, loss = 0.09660183\n",
      "Iteration 63, loss = 0.09656683\n",
      "Iteration 64, loss = 0.09651113\n",
      "Iteration 65, loss = 0.09648129\n",
      "Iteration 66, loss = 0.09636001\n",
      "Iteration 67, loss = 0.09626613\n",
      "Iteration 68, loss = 0.09626656\n",
      "Iteration 69, loss = 0.09621582\n",
      "Iteration 70, loss = 0.09609665\n",
      "Iteration 71, loss = 0.09598296\n",
      "Iteration 72, loss = 0.09610174\n",
      "Iteration 73, loss = 0.09590530\n",
      "Iteration 74, loss = 0.09580206\n",
      "Iteration 75, loss = 0.09563846\n",
      "Iteration 76, loss = 0.09569646\n",
      "Iteration 77, loss = 0.09557485\n",
      "Iteration 78, loss = 0.09560531\n",
      "Iteration 79, loss = 0.09551278\n",
      "Iteration 80, loss = 0.09553081\n",
      "Iteration 81, loss = 0.09559972\n",
      "Iteration 82, loss = 0.09535397\n",
      "Iteration 83, loss = 0.09525583\n",
      "Iteration 84, loss = 0.09525489\n",
      "Iteration 85, loss = 0.09515067\n",
      "Iteration 86, loss = 0.09519719\n",
      "Iteration 87, loss = 0.09492026\n",
      "Iteration 88, loss = 0.09517771\n",
      "Iteration 89, loss = 0.09498710\n",
      "Iteration 90, loss = 0.09502473\n",
      "Iteration 91, loss = 0.09500497\n",
      "Iteration 92, loss = 0.09496400\n",
      "Iteration 93, loss = 0.09490587\n",
      "Iteration 94, loss = 0.09488610\n",
      "Iteration 95, loss = 0.09473469\n",
      "Iteration 96, loss = 0.09471063\n",
      "Iteration 97, loss = 0.09481802\n",
      "Iteration 98, loss = 0.09465891\n",
      "Iteration 99, loss = 0.09464182\n",
      "Iteration 100, loss = 0.09458203\n",
      "Iteration 1, loss = 0.14939686\n",
      "Iteration 2, loss = 0.06635581\n",
      "Iteration 3, loss = 0.05545110\n",
      "Iteration 4, loss = 0.05174661\n",
      "Iteration 5, loss = 0.04972915\n",
      "Iteration 6, loss = 0.04834132\n",
      "Iteration 7, loss = 0.04723397\n",
      "Iteration 8, loss = 0.04619007\n",
      "Iteration 9, loss = 0.04515661\n",
      "Iteration 10, loss = 0.04446109\n",
      "Iteration 11, loss = 0.04385970\n",
      "Iteration 12, loss = 0.04338797\n",
      "Iteration 13, loss = 0.04314583\n",
      "Iteration 14, loss = 0.04271480\n",
      "Iteration 15, loss = 0.04247221\n",
      "Iteration 16, loss = 0.04215648\n",
      "Iteration 17, loss = 0.04202925\n",
      "Iteration 18, loss = 0.04184652\n",
      "Iteration 19, loss = 0.04153521\n",
      "Iteration 20, loss = 0.04143271\n",
      "Iteration 21, loss = 0.04128848\n",
      "Iteration 22, loss = 0.04115352\n",
      "Iteration 23, loss = 0.04093447\n",
      "Iteration 24, loss = 0.04079335\n",
      "Iteration 25, loss = 0.04073406\n",
      "Iteration 26, loss = 0.04056051\n",
      "Iteration 27, loss = 0.04045642\n",
      "Iteration 28, loss = 0.04039533\n",
      "Iteration 29, loss = 0.04030323\n",
      "Iteration 30, loss = 0.04015419\n",
      "Iteration 31, loss = 0.04017118\n",
      "Iteration 32, loss = 0.04001198\n",
      "Iteration 33, loss = 0.03996596\n",
      "Iteration 34, loss = 0.03988649\n",
      "Iteration 35, loss = 0.03968589\n",
      "Iteration 36, loss = 0.03965053\n",
      "Iteration 37, loss = 0.03946111\n",
      "Iteration 38, loss = 0.03907926\n",
      "Iteration 39, loss = 0.03869371\n",
      "Iteration 40, loss = 0.03819655\n",
      "Iteration 41, loss = 0.03783797\n",
      "Iteration 42, loss = 0.03748466\n",
      "Iteration 43, loss = 0.03725786\n",
      "Iteration 44, loss = 0.03703625\n",
      "Iteration 45, loss = 0.03690700\n",
      "Iteration 46, loss = 0.03675551\n",
      "Iteration 47, loss = 0.03665344\n",
      "Iteration 48, loss = 0.03662803\n",
      "Iteration 49, loss = 0.03641120\n",
      "Iteration 50, loss = 0.03630776\n",
      "Iteration 51, loss = 0.03636509\n",
      "Iteration 52, loss = 0.03627546\n",
      "Iteration 53, loss = 0.03617128\n",
      "Iteration 54, loss = 0.03609842\n",
      "Iteration 55, loss = 0.03608332\n",
      "Iteration 56, loss = 0.03604194\n",
      "Iteration 57, loss = 0.03598606\n",
      "Iteration 58, loss = 0.03594768\n",
      "Iteration 59, loss = 0.03601529\n",
      "Iteration 60, loss = 0.03585055\n",
      "Iteration 61, loss = 0.03576925\n",
      "Iteration 62, loss = 0.03582199\n",
      "Iteration 63, loss = 0.03575848\n",
      "Iteration 64, loss = 0.03573292\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03558362\n",
      "Iteration 2, loss = 0.01407097\n",
      "Iteration 3, loss = 0.01187067\n",
      "Iteration 4, loss = 0.01103456\n",
      "Iteration 5, loss = 0.01053743\n",
      "Iteration 6, loss = 0.01025084\n",
      "Iteration 7, loss = 0.01005268\n",
      "Iteration 8, loss = 0.00987130\n",
      "Iteration 9, loss = 0.00971659\n",
      "Iteration 10, loss = 0.00961043\n",
      "Iteration 11, loss = 0.00953381\n",
      "Iteration 12, loss = 0.00942967\n",
      "Iteration 13, loss = 0.00937050\n",
      "Iteration 14, loss = 0.00930116\n",
      "Iteration 15, loss = 0.00921898\n",
      "Iteration 16, loss = 0.00916294\n",
      "Iteration 17, loss = 0.00910785\n",
      "Iteration 18, loss = 0.00904891\n",
      "Iteration 19, loss = 0.00899059\n",
      "Iteration 20, loss = 0.00897251\n",
      "Iteration 21, loss = 0.00891123\n",
      "Iteration 22, loss = 0.00887051\n",
      "Iteration 23, loss = 0.00885485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21400589\n",
      "Iteration 2, loss = 0.14857323\n",
      "Iteration 3, loss = 0.13606477\n",
      "Iteration 4, loss = 0.13066388\n",
      "Iteration 5, loss = 0.12739107\n",
      "Iteration 6, loss = 0.12534814\n",
      "Iteration 7, loss = 0.12375258\n",
      "Iteration 8, loss = 0.12229063\n",
      "Iteration 9, loss = 0.12100369\n",
      "Iteration 10, loss = 0.12002400\n",
      "Iteration 11, loss = 0.11903093\n",
      "Iteration 12, loss = 0.11819342\n",
      "Iteration 13, loss = 0.11752877\n",
      "Iteration 14, loss = 0.11689048\n",
      "Iteration 15, loss = 0.11641997\n",
      "Iteration 16, loss = 0.11597827\n",
      "Iteration 17, loss = 0.11536983\n",
      "Iteration 18, loss = 0.11496071\n",
      "Iteration 19, loss = 0.11448054\n",
      "Iteration 20, loss = 0.11415422\n",
      "Iteration 21, loss = 0.11389820\n",
      "Iteration 22, loss = 0.11342928\n",
      "Iteration 23, loss = 0.11337182\n",
      "Iteration 24, loss = 0.11300907\n",
      "Iteration 25, loss = 0.11290029\n",
      "Iteration 26, loss = 0.11248340\n",
      "Iteration 27, loss = 0.11221287\n",
      "Iteration 28, loss = 0.11207369\n",
      "Iteration 29, loss = 0.11177840\n",
      "Iteration 30, loss = 0.11159550\n",
      "Iteration 31, loss = 0.11137734\n",
      "Iteration 32, loss = 0.11116217\n",
      "Iteration 33, loss = 0.11095449\n",
      "Iteration 34, loss = 0.11086793\n",
      "Iteration 35, loss = 0.11069851\n",
      "Iteration 36, loss = 0.11037354\n",
      "Iteration 37, loss = 0.11023043\n",
      "Iteration 38, loss = 0.10997745\n",
      "Iteration 39, loss = 0.10991833\n",
      "Iteration 40, loss = 0.10975611\n",
      "Iteration 41, loss = 0.10962153\n",
      "Iteration 42, loss = 0.10938093\n",
      "Iteration 43, loss = 0.10927340\n",
      "Iteration 44, loss = 0.10914956\n",
      "Iteration 45, loss = 0.10906038\n",
      "Iteration 46, loss = 0.10895347\n",
      "Iteration 47, loss = 0.10877655\n",
      "Iteration 48, loss = 0.10868550\n",
      "Iteration 49, loss = 0.10855865\n",
      "Iteration 50, loss = 0.10834685\n",
      "Iteration 51, loss = 0.10828849\n",
      "Iteration 52, loss = 0.10820340\n",
      "Iteration 53, loss = 0.10822712\n",
      "Iteration 54, loss = 0.10798869\n",
      "Iteration 55, loss = 0.10789939\n",
      "Iteration 56, loss = 0.10786405\n",
      "Iteration 57, loss = 0.10768800\n",
      "Iteration 58, loss = 0.10774424\n",
      "Iteration 59, loss = 0.10746223\n",
      "Iteration 60, loss = 0.10752605\n",
      "Iteration 61, loss = 0.10750621\n",
      "Iteration 62, loss = 0.10715705\n",
      "Iteration 63, loss = 0.10713567\n",
      "Iteration 64, loss = 0.10709894\n",
      "Iteration 65, loss = 0.10713823\n",
      "Iteration 66, loss = 0.10682385\n",
      "Iteration 67, loss = 0.10692525\n",
      "Iteration 68, loss = 0.10688519\n",
      "Iteration 69, loss = 0.10678509\n",
      "Iteration 70, loss = 0.10664201\n",
      "Iteration 71, loss = 0.10654541\n",
      "Iteration 72, loss = 0.10654031\n",
      "Iteration 73, loss = 0.10652618\n",
      "Iteration 74, loss = 0.10636743\n",
      "Iteration 75, loss = 0.10623423\n",
      "Iteration 76, loss = 0.10618589\n",
      "Iteration 77, loss = 0.10619927\n",
      "Iteration 78, loss = 0.10617858\n",
      "Iteration 79, loss = 0.10603468\n",
      "Iteration 80, loss = 0.10606534\n",
      "Iteration 81, loss = 0.10607217\n",
      "Iteration 82, loss = 0.10592642\n",
      "Iteration 83, loss = 0.10581929\n",
      "Iteration 84, loss = 0.10573516\n",
      "Iteration 85, loss = 0.10567812\n",
      "Iteration 86, loss = 0.10568182\n",
      "Iteration 87, loss = 0.10543917\n",
      "Iteration 88, loss = 0.10557110\n",
      "Iteration 89, loss = 0.10548221\n",
      "Iteration 90, loss = 0.10558285\n",
      "Iteration 91, loss = 0.10541417\n",
      "Iteration 92, loss = 0.10547216\n",
      "Iteration 93, loss = 0.10529930\n",
      "Iteration 94, loss = 0.10524367\n",
      "Iteration 95, loss = 0.10534229\n",
      "Iteration 96, loss = 0.10525428\n",
      "Iteration 97, loss = 0.10526000\n",
      "Iteration 98, loss = 0.10513141\n",
      "Iteration 99, loss = 0.10506475\n",
      "Iteration 100, loss = 0.10501768\n",
      "Iteration 1, loss = 0.15432081\n",
      "Iteration 2, loss = 0.07230073\n",
      "Iteration 3, loss = 0.06251602\n",
      "Iteration 4, loss = 0.05865812\n",
      "Iteration 5, loss = 0.05631168\n",
      "Iteration 6, loss = 0.05489770\n",
      "Iteration 7, loss = 0.05379500\n",
      "Iteration 8, loss = 0.05310991\n",
      "Iteration 9, loss = 0.05259724\n",
      "Iteration 10, loss = 0.05211940\n",
      "Iteration 11, loss = 0.05176039\n",
      "Iteration 12, loss = 0.05123528\n",
      "Iteration 13, loss = 0.05106333\n",
      "Iteration 14, loss = 0.05067994\n",
      "Iteration 15, loss = 0.05052574\n",
      "Iteration 16, loss = 0.05022427\n",
      "Iteration 17, loss = 0.04996987\n",
      "Iteration 18, loss = 0.04973088\n",
      "Iteration 19, loss = 0.04945552\n",
      "Iteration 20, loss = 0.04911855\n",
      "Iteration 21, loss = 0.04883295\n",
      "Iteration 22, loss = 0.04831952\n",
      "Iteration 23, loss = 0.04782413\n",
      "Iteration 24, loss = 0.04724081\n",
      "Iteration 25, loss = 0.04675033\n",
      "Iteration 26, loss = 0.04584432\n",
      "Iteration 27, loss = 0.04525256\n",
      "Iteration 28, loss = 0.04491934\n",
      "Iteration 29, loss = 0.04463590\n",
      "Iteration 30, loss = 0.04433297\n",
      "Iteration 31, loss = 0.04430076\n",
      "Iteration 32, loss = 0.04410620\n",
      "Iteration 33, loss = 0.04397881\n",
      "Iteration 34, loss = 0.04392308\n",
      "Iteration 35, loss = 0.04370626\n",
      "Iteration 36, loss = 0.04364597\n",
      "Iteration 37, loss = 0.04351535\n",
      "Iteration 38, loss = 0.04339004\n",
      "Iteration 39, loss = 0.04335452\n",
      "Iteration 40, loss = 0.04325652\n",
      "Iteration 41, loss = 0.04319627\n",
      "Iteration 42, loss = 0.04316060\n",
      "Iteration 43, loss = 0.04307126\n",
      "Iteration 44, loss = 0.04294384\n",
      "Iteration 45, loss = 0.04286620\n",
      "Iteration 46, loss = 0.04281172\n",
      "Iteration 47, loss = 0.04261770\n",
      "Iteration 48, loss = 0.04261949\n",
      "Iteration 49, loss = 0.04250982\n",
      "Iteration 50, loss = 0.04242086\n",
      "Iteration 51, loss = 0.04240801\n",
      "Iteration 52, loss = 0.04239845\n",
      "Iteration 53, loss = 0.04234191\n",
      "Iteration 54, loss = 0.04228075\n",
      "Iteration 55, loss = 0.04226740\n",
      "Iteration 56, loss = 0.04224466\n",
      "Iteration 57, loss = 0.04217984\n",
      "Iteration 58, loss = 0.04215923\n",
      "Iteration 59, loss = 0.04215920\n",
      "Iteration 60, loss = 0.04207383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03720912\n",
      "Iteration 2, loss = 0.01622950\n",
      "Iteration 3, loss = 0.01356369\n",
      "Iteration 4, loss = 0.01254365\n",
      "Iteration 5, loss = 0.01197730\n",
      "Iteration 6, loss = 0.01163997\n",
      "Iteration 7, loss = 0.01143545\n",
      "Iteration 8, loss = 0.01124278\n",
      "Iteration 9, loss = 0.01108901\n",
      "Iteration 10, loss = 0.01097029\n",
      "Iteration 11, loss = 0.01085588\n",
      "Iteration 12, loss = 0.01074218\n",
      "Iteration 13, loss = 0.01054613\n",
      "Iteration 14, loss = 0.01041173\n",
      "Iteration 15, loss = 0.01033057\n",
      "Iteration 16, loss = 0.01028171\n",
      "Iteration 17, loss = 0.01022711\n",
      "Iteration 18, loss = 0.01016275\n",
      "Iteration 19, loss = 0.01010568\n",
      "Iteration 20, loss = 0.01006670\n",
      "Iteration 21, loss = 0.01002368\n",
      "Iteration 22, loss = 0.00998666\n",
      "Iteration 23, loss = 0.00997370\n",
      "Iteration 24, loss = 0.00993482\n",
      "Iteration 25, loss = 0.00989960\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20707599\n",
      "Iteration 2, loss = 0.14142310\n",
      "Iteration 3, loss = 0.12822942\n",
      "Iteration 4, loss = 0.12220754\n",
      "Iteration 5, loss = 0.11848333\n",
      "Iteration 6, loss = 0.11589868\n",
      "Iteration 7, loss = 0.11386636\n",
      "Iteration 8, loss = 0.11248702\n",
      "Iteration 9, loss = 0.11118818\n",
      "Iteration 10, loss = 0.10995022\n",
      "Iteration 11, loss = 0.10873261\n",
      "Iteration 12, loss = 0.10784688\n",
      "Iteration 13, loss = 0.10721886\n",
      "Iteration 14, loss = 0.10641744\n",
      "Iteration 15, loss = 0.10597180\n",
      "Iteration 16, loss = 0.10553172\n",
      "Iteration 17, loss = 0.10492764\n",
      "Iteration 18, loss = 0.10443008\n",
      "Iteration 19, loss = 0.10399622\n",
      "Iteration 20, loss = 0.10356956\n",
      "Iteration 21, loss = 0.10333693\n",
      "Iteration 22, loss = 0.10273754\n",
      "Iteration 23, loss = 0.10264917\n",
      "Iteration 24, loss = 0.10227810\n",
      "Iteration 25, loss = 0.10209605\n",
      "Iteration 26, loss = 0.10169713\n",
      "Iteration 27, loss = 0.10140587\n",
      "Iteration 28, loss = 0.10109077\n",
      "Iteration 29, loss = 0.10096720\n",
      "Iteration 30, loss = 0.10078888\n",
      "Iteration 31, loss = 0.10045803\n",
      "Iteration 32, loss = 0.10025927\n",
      "Iteration 33, loss = 0.10002163\n",
      "Iteration 34, loss = 0.09984317\n",
      "Iteration 35, loss = 0.09977001\n",
      "Iteration 36, loss = 0.09939981\n",
      "Iteration 37, loss = 0.09927895\n",
      "Iteration 38, loss = 0.09910168\n",
      "Iteration 39, loss = 0.09910193\n",
      "Iteration 40, loss = 0.09886536\n",
      "Iteration 41, loss = 0.09882738\n",
      "Iteration 42, loss = 0.09863236\n",
      "Iteration 43, loss = 0.09843100\n",
      "Iteration 44, loss = 0.09836182\n",
      "Iteration 45, loss = 0.09831160\n",
      "Iteration 46, loss = 0.09824167\n",
      "Iteration 47, loss = 0.09816293\n",
      "Iteration 48, loss = 0.09807111\n",
      "Iteration 49, loss = 0.09785708\n",
      "Iteration 50, loss = 0.09776901\n",
      "Iteration 51, loss = 0.09763689\n",
      "Iteration 52, loss = 0.09774704\n",
      "Iteration 53, loss = 0.09775479\n",
      "Iteration 54, loss = 0.09747646\n",
      "Iteration 55, loss = 0.09749609\n",
      "Iteration 56, loss = 0.09747350\n",
      "Iteration 57, loss = 0.09730046\n",
      "Iteration 58, loss = 0.09733482\n",
      "Iteration 59, loss = 0.09705532\n",
      "Iteration 60, loss = 0.09713754\n",
      "Iteration 61, loss = 0.09708045\n",
      "Iteration 62, loss = 0.09684724\n",
      "Iteration 63, loss = 0.09672075\n",
      "Iteration 64, loss = 0.09672765\n",
      "Iteration 65, loss = 0.09674481\n",
      "Iteration 66, loss = 0.09654244\n",
      "Iteration 67, loss = 0.09658290\n",
      "Iteration 68, loss = 0.09660995\n",
      "Iteration 69, loss = 0.09651028\n",
      "Iteration 70, loss = 0.09635266\n",
      "Iteration 71, loss = 0.09621810\n",
      "Iteration 72, loss = 0.09628422\n",
      "Iteration 73, loss = 0.09618553\n",
      "Iteration 74, loss = 0.09614705\n",
      "Iteration 75, loss = 0.09595635\n",
      "Iteration 76, loss = 0.09594071\n",
      "Iteration 77, loss = 0.09593393\n",
      "Iteration 78, loss = 0.09588091\n",
      "Iteration 79, loss = 0.09591935\n",
      "Iteration 80, loss = 0.09583417\n",
      "Iteration 81, loss = 0.09580513\n",
      "Iteration 82, loss = 0.09575035\n",
      "Iteration 83, loss = 0.09568208\n",
      "Iteration 84, loss = 0.09562627\n",
      "Iteration 85, loss = 0.09553378\n",
      "Iteration 86, loss = 0.09557071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.14583915\n",
      "Iteration 2, loss = 0.05970118\n",
      "Iteration 3, loss = 0.04949844\n",
      "Iteration 4, loss = 0.04637550\n",
      "Iteration 5, loss = 0.04438947\n",
      "Iteration 6, loss = 0.04309097\n",
      "Iteration 7, loss = 0.04203553\n",
      "Iteration 8, loss = 0.04151593\n",
      "Iteration 9, loss = 0.04095702\n",
      "Iteration 10, loss = 0.04054682\n",
      "Iteration 11, loss = 0.04021102\n",
      "Iteration 12, loss = 0.03978754\n",
      "Iteration 13, loss = 0.03950495\n",
      "Iteration 14, loss = 0.03915858\n",
      "Iteration 15, loss = 0.03877766\n",
      "Iteration 16, loss = 0.03837508\n",
      "Iteration 17, loss = 0.03764260\n",
      "Iteration 18, loss = 0.03696406\n",
      "Iteration 19, loss = 0.03642745\n",
      "Iteration 20, loss = 0.03611820\n",
      "Iteration 21, loss = 0.03589077\n",
      "Iteration 22, loss = 0.03573759\n",
      "Iteration 23, loss = 0.03550830\n",
      "Iteration 24, loss = 0.03541816\n",
      "Iteration 25, loss = 0.03519708\n",
      "Iteration 26, loss = 0.03506501\n",
      "Iteration 27, loss = 0.03496723\n",
      "Iteration 28, loss = 0.03488592\n",
      "Iteration 29, loss = 0.03470963\n",
      "Iteration 30, loss = 0.03459456\n",
      "Iteration 31, loss = 0.03457011\n",
      "Iteration 32, loss = 0.03442486\n",
      "Iteration 33, loss = 0.03442569\n",
      "Iteration 34, loss = 0.03432039\n",
      "Iteration 35, loss = 0.03427777\n",
      "Iteration 36, loss = 0.03420094\n",
      "Iteration 37, loss = 0.03414152\n",
      "Iteration 38, loss = 0.03408490\n",
      "Iteration 39, loss = 0.03395419\n",
      "Iteration 40, loss = 0.03397720\n",
      "Iteration 41, loss = 0.03393996\n",
      "Iteration 42, loss = 0.03385427\n",
      "Iteration 43, loss = 0.03380919\n",
      "Iteration 44, loss = 0.03379806\n",
      "Iteration 45, loss = 0.03368762\n",
      "Iteration 46, loss = 0.03370729\n",
      "Iteration 47, loss = 0.03361515\n",
      "Iteration 48, loss = 0.03362727\n",
      "Iteration 49, loss = 0.03349886\n",
      "Iteration 50, loss = 0.03343377\n",
      "Iteration 51, loss = 0.03349425\n",
      "Iteration 52, loss = 0.03349722\n",
      "Iteration 53, loss = 0.03342299\n",
      "Iteration 54, loss = 0.03335515\n",
      "Iteration 55, loss = 0.03335973\n",
      "Iteration 56, loss = 0.03334997\n",
      "Iteration 57, loss = 0.03328939\n",
      "Iteration 58, loss = 0.03328920\n",
      "Iteration 59, loss = 0.03332831\n",
      "Iteration 60, loss = 0.03318801\n",
      "Iteration 61, loss = 0.03314573\n",
      "Iteration 62, loss = 0.03316623\n",
      "Iteration 63, loss = 0.03309262\n",
      "Iteration 64, loss = 0.03309996\n",
      "Iteration 65, loss = 0.03304938\n",
      "Iteration 66, loss = 0.03303133\n",
      "Iteration 67, loss = 0.03300659\n",
      "Iteration 68, loss = 0.03299450\n",
      "Iteration 69, loss = 0.03289674\n",
      "Iteration 70, loss = 0.03290643\n",
      "Iteration 71, loss = 0.03290095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03545524\n",
      "Iteration 2, loss = 0.01401520\n",
      "Iteration 3, loss = 0.01147717\n",
      "Iteration 4, loss = 0.01049406\n",
      "Iteration 5, loss = 0.00997929\n",
      "Iteration 6, loss = 0.00968196\n",
      "Iteration 7, loss = 0.00944379\n",
      "Iteration 8, loss = 0.00923726\n",
      "Iteration 9, loss = 0.00908776\n",
      "Iteration 10, loss = 0.00896797\n",
      "Iteration 11, loss = 0.00884333\n",
      "Iteration 12, loss = 0.00872857\n",
      "Iteration 13, loss = 0.00860885\n",
      "Iteration 14, loss = 0.00853098\n",
      "Iteration 15, loss = 0.00844555\n",
      "Iteration 16, loss = 0.00838957\n",
      "Iteration 17, loss = 0.00831419\n",
      "Iteration 18, loss = 0.00825135\n",
      "Iteration 19, loss = 0.00820550\n",
      "Iteration 20, loss = 0.00817872\n",
      "Iteration 21, loss = 0.00813537\n",
      "Iteration 22, loss = 0.00810309\n",
      "Iteration 23, loss = 0.00807361\n",
      "Iteration 24, loss = 0.00805190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "                               RMSE                           MAE            \\\n",
      "Target Variable             OffsetX   OffsetY    Volume   OffsetX   OffsetY   \n",
      "Filter + Model                                                                \n",
      "DTCWT + Bilateral          0.254026  0.128398  0.419616  0.198799  0.099476   \n",
      "DTCWT + Gaussian           0.290186  0.140737  0.456065  0.227281  0.109642   \n",
      "DTCWT + Median             0.326568  0.160317  0.488681  0.255435  0.125064   \n",
      "DTCWT + Multivariate       0.265455  0.132661  0.433538  0.208834  0.103143   \n",
      "DTCWT + Savitzky-Golay     0.246714  0.124212  0.399829  0.192595  0.094822   \n",
      "DTCWT + Wavelet Denoising  0.256433  0.125707  0.440767  0.199344  0.097346   \n",
      "\n",
      "                                     \n",
      "Target Variable              Volume  \n",
      "Filter + Model                       \n",
      "DTCWT + Bilateral          0.315408  \n",
      "DTCWT + Gaussian           0.346343  \n",
      "DTCWT + Median             0.373315  \n",
      "DTCWT + Multivariate       0.328047  \n",
      "DTCWT + Savitzky-Golay     0.301028  \n",
      "DTCWT + Wavelet Denoising  0.333887  \n"
     ]
    }
   ],
   "source": [
    "# Filtering functions\n",
    "def apply_dtcwt_filter(signal):\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=1)\n",
    "    coeffs[1:] = [np.zeros_like(i) for i in coeffs[1:]]\n",
    "    return pywt.waverec(coeffs, 'db4')[:len(signal)]\n",
    "\n",
    "def apply_savgol_filter(signal):\n",
    "    return savgol_filter(signal, window_length=min(len(signal), 21), polyorder=2)\n",
    "\n",
    "def apply_bilateral_filter(signal):\n",
    "    return bilateralFilter(signal.astype(np.float32), 9, 75, 75)\n",
    "\n",
    "def apply_median_filter(signal):\n",
    "    return medfilt(signal, kernel_size=3)\n",
    "\n",
    "def apply_multivariate_filter(signal):\n",
    "    return np.convolve(signal, np.ones(5)/5, mode='same')\n",
    "\n",
    "def apply_gaussian_filter(signal):\n",
    "    return gaussian_filter(signal, sigma=1)\n",
    "\n",
    "def apply_wavelet_denoising(signal):\n",
    "    return denoise_wavelet(signal, method='BayesShrink', mode='soft', rescale_sigma=True)\n",
    "\n",
    "# Filtering options\n",
    "filters = {\n",
    "    \"DTCWT + Savitzky-Golay\": lambda x: apply_savgol_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Bilateral\": lambda x: apply_bilateral_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Median\": lambda x: apply_median_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Multivariate\": lambda x: apply_multivariate_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Gaussian\": lambda x: apply_gaussian_filter(apply_dtcwt_filter(x)),\n",
    "    \"DTCWT + Wavelet Denoising\": lambda x: apply_wavelet_denoising(apply_dtcwt_filter(x))\n",
    "}\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "for filter_name, filter_func in filters.items():\n",
    "    filtered_data = y.copy()\n",
    "    for col in targets:\n",
    "        filtered_data[col] = filter_func(y[col])\n",
    "\n",
    "    # Standardization\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_train_scaled = scaler_y.fit_transform(filtered_data.loc[y_train.index])\n",
    "    y_test_scaled = scaler_y.transform(filtered_data.loc[y_test.index])\n",
    "\n",
    "    # Train ANN model\n",
    "    for i, target in enumerate(targets):\n",
    "        ann_model = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=100, random_state=42,verbose=True)\n",
    "        ann_model.fit(X_train_scaled, y_train_scaled[:, i])\n",
    "\n",
    "        y_pred_ann = ann_model.predict(X_test_scaled)\n",
    "        rmse_ann = np.sqrt(mean_squared_error(y_test_scaled[:, i], y_pred_ann))\n",
    "        mae_ann = mean_absolute_error(y_test_scaled[:, i], y_pred_ann)\n",
    "\n",
    "        results.append([filter_name, target, rmse_ann, mae_ann])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Filter + Model', 'Target Variable', 'RMSE', 'MAE'])\n",
    "\n",
    "# Print results in a structured way\n",
    "print(results_df.pivot(index='Filter + Model', columns='Target Variable', values=['RMSE', 'MAE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
